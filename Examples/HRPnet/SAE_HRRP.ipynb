{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from misc import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义稀疏自编码器\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, sparsity_param=0.05, beta=3):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.sparsity_param = sparsity_param  # 稀疏性目标\n",
    "        self.beta = beta  # KL散度惩罚项的权重\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        output = self.decoder(hidden)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积稀疏自编码器\n",
    "class ConvSparseAutoencoder(nn.Module):\n",
    "    def __init__(self, sparsity_param=0.05, beta=3):\n",
    "        super(ConvSparseAutoencoder, self).__init__()\n",
    "        \n",
    "        # 编码器部分：卷积层提取特征\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),  # 1个输入通道（如灰度图），16个输出特征图\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # 16个输入特征图，32个输出特征图\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 解码器部分：反卷积（转置卷积）层重建输入\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # 从32个特征图还原到16个\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  # 从16个特征图还原到1个通道\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.sparsity_param = sparsity_param  # 稀疏性目标\n",
    "        self.beta = beta  # KL散度惩罚项的权重\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        output = self.decoder(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def kl_divergence(self, rho, rho_hat):\n",
    "        \"\"\"KL散度，用于稀疏性惩罚\"\"\"\n",
    "        rho = torch.tensor(rho)\n",
    "        rho_hat = torch.mean(rho_hat, dim=0)\n",
    "        return torch.sum(rho * torch.log(rho / rho_hat) + (1 - rho) * torch.log((1 - rho) / (1 - rho_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'G:/MRM_0.5/'\n",
    "train_data_path = dataset_path + 'train/'\n",
    "test_data_path = dataset_path + 'test/'\n",
    "\n",
    "def preprocessing(data, snr, pad_size, sparse_size = 8):\n",
    "\n",
    "    hrrp, noise_power = awgn(data['hrrp']['HH'], snr=snr)\n",
    "    hrrp = fftshift(ifft(hrrp,axis = 0),axes=0)\n",
    "    hrrp = np.log10(np.abs(hrrp))\n",
    "    return normalize(hrrp).astype(np.float32)\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, dataset_dir, snr, pad_size):\n",
    "        self.snr = snr\n",
    "        self.pad_size = pad_size\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.instance_list = self.get_instance()\n",
    "    \n",
    "    def get_instance(self):\n",
    "        instance_list = []\n",
    "        for label in os.listdir(self.dataset_dir):\n",
    "            label_dir = os.path.join(self.dataset_dir,label)\n",
    "            label_list = glob.glob(label_dir+'/*.pkl')\n",
    "            instance_list += label_list\n",
    "        return instance_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instance_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = load_pkl(self.instance_list[idx])\n",
    "        x = preprocessing(data, snr = self.snr, pad_size= self.pad_size)  \n",
    "        y = data['target_id']\n",
    "        return x, torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = 0\n",
    "pad_size = 201\n",
    "train_dataset = Dataset(train_data_path, snr = snr, pad_size = pad_size)\n",
    "test_dataset = Dataset(test_data_path, snr = snr, pad_size = pad_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "inp, label = train_dataset.__getitem__(0)\n",
    "\n",
    "inp = torch.from_numpy(inp).to(device).reshape(-1)\n",
    "# print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义稀疏自编码器模型\n",
    "input_size = inp.shape[0]\n",
    "\n",
    "hidden_size = 128  # 隐藏层大小\n",
    "model = SparseAutoencoder(input_size, hidden_size).to(device)\n",
    "\n",
    "outputs, hidden = model.forward(inp.unsqueeze(0))\n",
    "\n",
    "\n",
    "def l1_sparsity_loss(hidden):\n",
    "    # 计算隐藏层激活值的 L1 正则化\n",
    "    return torch.sum(torch.abs(hidden))\n",
    "\n",
    "# 定义损失函数，包括 MSE、L2 正则化 和 稀疏性正则化\n",
    "def mse_sae_loss(outputs, inputs, model, hidden, lambda_l2, beta_sparsity, sparsity_param=0.05):\n",
    "    # 计算 MSE 损失\n",
    "    mse_loss = nn.MSELoss()(outputs, inputs)\n",
    "    \n",
    "    # 计算 L2 正则化（权重正则化）\n",
    "    l2_reg = 0\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.sum(param ** 2)\n",
    "    \n",
    "    # L1 稀疏性正则化\n",
    "    # sparsity_loss = l1_sparsity_loss(hidden)\n",
    "    \n",
    "    # 总损失 = MSE + λ · L2 正则化 + β · L1 稀疏性正则化\n",
    "    total_loss = mse_loss + lambda_l2 * l2_reg \n",
    "    # + beta_sparsity * sparsity_loss\n",
    "    return total_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 433141.2793\n",
      "Epoch [2/1000], Loss: 338.8436\n",
      "Early stopping triggered at epoch 2\n"
     ]
    }
   ],
   "source": [
    "# 定义优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "mse_loss = nn.MSELoss()\n",
    "num_epochs = 1000\n",
    "lambda_l2 = 0.5\n",
    "beta_sparsity = 0.05\n",
    "early_stop_threshold = 0\n",
    "early_stop_patience = 1\n",
    "\n",
    "# 稀疏自编码器训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, _ in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), -1).to(device)  # 展平输入图像\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs, hidden = model(inputs)\n",
    "        # 重构损失（MSE）\n",
    "        loss = mse_sae_loss(outputs, inputs, model, hidden, lambda_l2, beta_sparsity, sparsity_param=0.05)\n",
    "        # 反向传播与优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
    "\n",
    "    # 早停判断\n",
    "    if total_loss < early_stop_threshold:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    else:\n",
    "        early_stop_counter = 0  # 重置计数器\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'SAE_HRRP.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, _ in train_loader:\n",
    "    model.eval()\n",
    "    inputs = inputs.view(inputs.size(0), -1).to(device)\n",
    "    outputs, hiddens = model(inputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "        ...,\n",
       "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
       "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

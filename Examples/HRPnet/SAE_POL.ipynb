{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from misc import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义稀疏自编码器\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, sparsity_param=0.05, beta=3):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_size, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.sparsity_param = sparsity_param  # 稀疏性目标\n",
    "        self.beta = beta  # KL散度惩罚项的权重\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        output = self.decoder(hidden)\n",
    "        return output, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'G:/MRM_0.5/'\n",
    "train_data_path = dataset_path + 'train/'\n",
    "test_data_path = dataset_path + 'test/'\n",
    "\n",
    "def preprocessing(data, snr, pad_size, sparse_size = 8):\n",
    "    E_pol = []\n",
    "    polar_type = ['HH', 'HV', 'VH', 'VV']\n",
    "    E, noise_power = awgn(data['echo']['HH'], snr=snr)\n",
    "    for pol in polar_type:\n",
    "        E, _ = awgnfp(data['echo'][pol], noise_power=noise_power)\n",
    "        E_pol.append(np.abs(E))\n",
    "    E_pol = np.array(E_pol).astype(np.float32)       # (4, 512)\n",
    "    return E_pol\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, dataset_dir, snr, pad_size):\n",
    "        self.snr = snr\n",
    "        self.pad_size = pad_size\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.instance_list = self.get_instance()\n",
    "    \n",
    "    def get_instance(self):\n",
    "        instance_list = []\n",
    "        for label in os.listdir(self.dataset_dir):\n",
    "            label_dir = os.path.join(self.dataset_dir,label)\n",
    "            label_list = glob.glob(label_dir+'/*.pkl')\n",
    "            instance_list += label_list\n",
    "        return instance_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instance_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = load_pkl(self.instance_list[idx])\n",
    "        x = preprocessing(data, snr = self.snr, pad_size= self.pad_size)  \n",
    "        y = data['target_id']\n",
    "        return x, torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "snr = 0\n",
    "pad_size = 201\n",
    "train_dataset = Dataset(train_data_path, snr = snr, pad_size = pad_size)\n",
    "test_dataset = Dataset(test_data_path, snr = snr, pad_size = pad_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "inp, label = train_dataset.__getitem__(0)\n",
    "inp = torch.from_numpy(inp).to(device)\n",
    "print(inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义稀疏自编码器模型\n",
    "input_size = inp.shape[0]*inp.shape[1]  \n",
    "\n",
    "hidden_size = 128  # 隐藏层大小\n",
    "model = SparseAutoencoder(input_size, hidden_size).to(device)\n",
    "\n",
    "outputs, hidden = model.forward(inp.unsqueeze(0).view(1, 2048))\n",
    "\n",
    "\n",
    "def l1_sparsity_loss(hidden):\n",
    "    # 计算隐藏层激活值的 L1 正则化\n",
    "    return torch.sum(torch.abs(hidden))\n",
    "\n",
    "# 定义损失函数，包括 MSE、L2 正则化 和 稀疏性正则化\n",
    "def mse_sae_loss(outputs, inputs, model, hidden, lambda_l2, beta_sparsity, sparsity_param=0.05):\n",
    "    # 计算 MSE 损失\n",
    "    mse_loss = nn.MSELoss()(outputs, inputs)\n",
    "    \n",
    "    # 计算 L2 正则化（权重正则化）\n",
    "    l2_reg = 0\n",
    "    for param in model.parameters():\n",
    "        l2_reg += torch.sum(param ** 2)\n",
    "    \n",
    "    # L1 稀疏性正则化\n",
    "    sparsity_loss = l1_sparsity_loss(hidden)\n",
    "    \n",
    "    # 总损失 = MSE + λ · L2 正则化 + β · L1 稀疏性正则化\n",
    "    total_loss = mse_loss + lambda_l2 * l2_reg + beta_sparsity * sparsity_loss\n",
    "    return total_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 10811.6551\n",
      "Epoch [2/1000], Loss: 900.6318\n",
      "Epoch [3/1000], Loss: 383.1675\n",
      "Epoch [4/1000], Loss: 155.6699\n",
      "Epoch [5/1000], Loss: 78.8966\n",
      "Epoch [6/1000], Loss: 47.4071\n",
      "Early stopping triggered at epoch 6\n"
     ]
    }
   ],
   "source": [
    "# 定义优化器和损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "mse_loss = nn.MSELoss()\n",
    "num_epochs = 1000\n",
    "lambda_l2 = 0.5\n",
    "beta_sparsity = 0.5\n",
    "early_stop_threshold = 50\n",
    "early_stop_patience = 1\n",
    "\n",
    "# 稀疏自编码器训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for inputs, _ in train_loader:\n",
    "        inputs = inputs.view(inputs.size(0), -1).to(device)  # 展平输入图像\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs, hidden = model(inputs)\n",
    "        # 重构损失（MSE）\n",
    "        loss = mse_sae_loss(outputs, inputs, model, hidden, lambda_l2, beta_sparsity, sparsity_param=0.05)\n",
    "        # 反向传播与优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
    "\n",
    "    # 早停判断\n",
    "    if total_loss < early_stop_threshold:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    else:\n",
    "        early_stop_counter = 0  # 重置计数器\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'SAE_.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
